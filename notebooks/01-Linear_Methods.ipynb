{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:32px; font-weight: bolder; text-align: center\"> Supervised and unsupervised <br/> learning with linear methods </p>\n",
    "<p style=\"text-align: center\"><i> authored by: <a href=\"mailto:michele.ceriotti@gmail.com\"> Michele Ceriotti </a></i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an introduction to ridge regression and principal component analysis, both in terms of a simple 1D regression problem, and with an application to a small database of inorganic materials. \n",
    "\n",
    "_References:_\n",
    "- [Nature 559, 547â€“555 (2018)](https://www.nature.com/articles/s41586-018-0337-2)\n",
    "- [J. Chem. Phys. 150, 150901 (2019)](https://doi.org/10.1063/1.5091842)\n",
    "- [Springer Vol. 4. No. 4. (2006)](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and dependencies\n",
    "This module uses `sklearn` to perform regression, and `chemiscope` to visualize structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scicode-widgets imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import chemiscope\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import FloatSlider, IntSlider, Checkbox, Dropdown, HBox, Layout, HTML\n",
    "\n",
    "from markdown import markdown as mdwn\n",
    "\n",
    "import scwidgets\n",
    "from scwidgets.check import (\n",
    "    Check,\n",
    "    CheckRegistry,\n",
    "    assert_numpy_allclose,\n",
    "    assert_numpy_floating_sub_dtype,\n",
    "    assert_shape,\n",
    "    assert_type,\n",
    ")\n",
    "from scwidgets.code import ParametersPanel, CodeInput\n",
    "from scwidgets.cue import CueObject, CueFigure\n",
    "from scwidgets.exercise import CodeExercise, TextExercise, ExerciseRegistry\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'widget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import ase\n",
    "import functools\n",
    "import copy\n",
    "from ase.io import read, write\n",
    "from ase.calculators import lj, eam\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_registry = ExerciseRegistry(filename_prefix=\"module_01\")\n",
    "exercise_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_registry = CheckRegistry()\n",
    "check_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_summary = TextExercise(\n",
    "    description=\"\"\"You can use this box to make general considerations, \n",
    "    or keep track of your doubts and questions about this notebook.\"\"\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    title=\"Module comments\",\n",
    "    key=\"00\"\n",
    ")\n",
    "display(module_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-driven\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-driven modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a very brief and over-simplified primer on \"data-driven\" modeling. \n",
    "In abstract terms, a data-driven approach attempts to establish a relationship between _input_ data and _target_ properties by recognizing or \"learning\" patterns in the data itself instead of using deductive reasoning, i.e. without proceeding though a series of logical steps starting from an hypothesis on the physical behavior of a system. \n",
    "\n",
    "Once a generic, mathematically flexible model has been chosen, the empirical association between inputs and targets is taken as the only basis to establish an _inductive_ relationship between them: we only look at what the data tells to be a strong correlation, neither reasoning on causal links, nor attempting to develop a simple, coherent, or elegant theory.\n",
    "\n",
    "The traditional scientific method proceeds through a combination of induction and deduction, while data-driven approaches are intended to be entirely inductive. On the risks of purely inductive reasoning, see [Bertrand Russel's inductivist chicken story](http://www.ditext.com/russell/rus6.html). \n",
    "\n",
    "In practice, _inductive biases_ are often included in the modeling, by means of the choices that are made in the construction and the tuning of the model itself: this is how a component of physics-inspired (deductive) concepts can make it back into machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the most primitive data-driven model, consider the case of _linear regression_. \n",
    "A set of $n_\\mathrm{train}$ data points and targets $\\{x_i, y_i\\}_{i=1}^{n_\\mathrm{train}} $ are assumed to follow a linear relationship of the form $y(x)=a x$, where the slope $a$ is an adjustable parameter. \n",
    "For a given value of $a$, one can compute the _loss_, which measures the discrepancy between the true value of the targets and the predictions of the model. Here, we take the mean square error ($L^2$ norm)\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i^{n_\\mathrm{train}} (y(x_i)-y_i)^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This widget allows you to play around with the core idea of linear regression: by adjusting the value of $a$ you can minimize the discrepancy between predictions and targets, and find the best model within the class chosen to represent the input-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "lr_demo_figure, _ = plt.subplots(1, 1, tight_layout=True)\n",
    "\n",
    "lr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "lr_y = 2.33*lr_x+(np.random.uniform(size=20)-0.5)*2\n",
    "def lr_update(code_exercise):\n",
    "    a = code_exercise.parameters[\"a\"]\n",
    "    ax = code_exercise.figure.get_axes()[0]\n",
    "    ax.plot(lr_x, lr_y, 'b.')\n",
    "    ax.plot([-5,5],[-5*a,5*a], 'r--')\n",
    "    l = np.mean((lr_y-a*lr_x)**2)\n",
    "    ax.text(-4,8,fr'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "\n",
    "lr_parameters={\n",
    "        \"a\": FloatSlider(\n",
    "            value=1, min=-5, max=5, step=0.1, description=r\"Slope $a$\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "lr_code_demo = CodeExercise(\n",
    "    parameters=lr_parameters,\n",
    "    outputs=lr_demo_figure,\n",
    "    update=lr_update,\n",
    "    update_mode=\"continuous\"\n",
    ")\n",
    "lr_code_demo.run_update()\n",
    "display(lr_code_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex01_txt = TextExercise(\n",
    "    description=\"What is (roughly) the best value of $a$ that minimizes the loss in the linear regression model?\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"01\",\n",
    "    title=\"Exercise 01: Best fit parameter\"\n",
    ")\n",
    "display(ex01_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression model, the loss can be minimized with a closed expression, by setting $\\partial \\ell/\\partial a = 0$ and solving for $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex02_txt = TextExercise(\n",
    "    description=\"Write the expression for the optimal $a$ for a one-dimensional linear regression problem where the loss is optimized on pairs of inputs and targets $(x_i, y_i)$\",\n",
    "    key=\"02\",\n",
    "    title=\"Exercise 02: Analytical least-square fit\",\n",
    "    exercise_registry=exercise_registry,\n",
    ")\n",
    "display(ex02_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be easily generalized to more complex models: in the most general terms, $\\ell$ can be minimized numerically, by computing the derivatives of $y(x)$ with respect to the model parameters. \n",
    "Here, we consider the simpler case of a polynomial model, in which $y(x)=\\sum_k^{d} w_k x^k$. \n",
    "\n",
    "\n",
    "_NB: this is a very bad choice of a polynomial basis to expand the function (most notably, because the different polynomials are not orthogonal). We are just doing this as a simple example, never try this for a real problem!_\n",
    "\n",
    "This can actually be seen as a special case of multi-dimensional linear regression, where each sample is described by several _features_ (or often referred to as _descriptors_ in material science). In this case we would have a $d$-dimensional feature vector of form $x_{ik}=x_i^{k}$ and can recover the target as a vector dot product $y(\\mathbf{x}_i) = \\mathbf{w}\\cdot\\mathbf{x}_i$.\n",
    "\n",
    "\n",
    "Play around with the widget below. It is _really_ difficult to fit the model manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the polynomial target function\n",
    "npoly = 5\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "\n",
    "np.random.seed(12345)\n",
    "pr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "pr_y = (np.random.uniform(size=20)-0.5)*3\n",
    "for k in range(len(pr_w)):\n",
    "    pr_y += pr_w[k]*pr_x**k\n",
    "pr_demo_figure, _ = plt.subplots(1, 1, tight_layout=True)\n",
    "\n",
    "def pr_update(cue_exercise):    \n",
    "    w_0,w_1,w_2,w_3,w_4,w_5 = cue_exercise.parameters.values()\n",
    "    ax = cue_exercise.figure.get_axes()[0]    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    lw = [w_0,w_1,w_2,w_3,w_4,w_5]\n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    for k in range(len(lw)):\n",
    "        yy += lw[k]*xx**k\n",
    "        my += lw[k]*pr_x**k\n",
    "        ty += pr_w[k]*xx**k\n",
    "    \n",
    "    l = np.mean((pr_y-my)**2)\n",
    "    ax.plot(pr_x, pr_y, 'b.', label=\"train data\")\n",
    "    ax.plot(xx, yy, 'r--', label=\"manual fit\")\n",
    "    ax.text(-4,-1,fr'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_ylim(min(pr_y)-1, max(pr_y)+1)\n",
    "    \n",
    "pr_pb = ParametersPanel(\n",
    "    w_0=FloatSlider(value=1.0, min=-5.0, max=5.0, step=0.01,  description=r'$w_0$'),\n",
    "    w_1=FloatSlider(value=0.01, min=-2.0, max=2.0, step=0.01,  description=r'$w_1$'),\n",
    "    w_2=FloatSlider(value=0.01, min=-1.0, max=1.0, step=0.01, description=r'$w_2$'),\n",
    "    w_3=FloatSlider(value=-0.2, min=-1.0, max=1.0, step=0.01, description=r'$w_3$'),\n",
    "    w_4=FloatSlider(value=0.01, min=-0.1, max=0.1, step=0.01, description=r'$w_4$'),\n",
    "    w_5=FloatSlider(value=0.01, min=-0.1, max=0.1, step=0.01, description=r'$w_5$')\n",
    ")\n",
    "polynomial_regression_code_demo = CodeExercise(\n",
    "    parameters=pr_pb,\n",
    "    outputs=pr_demo_figure,\n",
    "    update=pr_update,\n",
    "    update_mode=\"continuous\"\n",
    ")\n",
    "\n",
    "polynomial_regression_code_demo.run_update()\n",
    "display(polynomial_regression_code_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss can be written in a vectorial form, $\\ell(\\mathbf{x}_i, y_i) \\propto \\sum_i (\\mathbf{w}\\cdot\\mathbf{x}_i - y_i)^2$. If $\\mathbf{X}\\in\\mathbb{R}^{n_\\mathrm{train}\\times d}$ is the matrix collecting the $x_i^k\\in\\mathbb{R}$ in the rows, as it is $\\mathbf{y}\\in\\mathbb{R}^{n_\\mathrm{train}}$ for the targets, then a closed expression for the optimal weight vector can be derived as\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\\\\\n",
    "$$\n",
    "See [here](https://en.wikipedia.org/wiki/Linear_regression#Least-squares_estimation_and_related_techniques) if you are interested in the derivation.\n",
    "You can compare the expression with the one-dimensional case, and you will see immediately how this expression generalizes the $d$-dimensional case.\n",
    "\n",
    "We can now start looking to more realistic issues that arise in the context of regression models. First, data can contain a certain level of _noise_. This can be actual random noise, or (often) hidden input features or relationships that cannot be captured by the chosen model. Second, a model that predicts the targets only for the data it had been trained on is of very little use: we want to be able to do predictions on unseen data.\n",
    "For this reason, it is customary to set aside a fraction of the available data that is not used to determine the weights in the minimization of $\\ell$. The error on this _test set_ is an indication of how good the model will perform on the prediction of new points, i.e. how well the model _generalizes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_loss_figure, _ = plt.subplots(1, 1, tight_layout=True)\n",
    "\n",
    "def pr_loss_update(code_exercise):    \n",
    "    # npoints are training points\n",
    "    ax = code_exercise.figure.get_axes()[0]\n",
    "    noise, hidden, npoints, tgt, fit = code_exercise.parameters.values()\n",
    "    \n",
    "    min_npoints = 10\n",
    "    max_npoints = 150\n",
    "    ntest = 50\n",
    "    poly_degree = 6\n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    np.random.seed(54321)\n",
    "    pr_x = (np.random.uniform(size=max_npoints+ntest)-0.5)*10\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(poly_degree)]).T\n",
    "    pr_y = (np.random.uniform(size=len(pr_x))-0.5)*noise\n",
    "    for k in range(len(pr_w)):\n",
    "        pr_y += pr_w[k]*pr_x**k\n",
    "    pr_y += hidden*np.sin(pr_x*4)\n",
    "    \n",
    "    fit_w = np.linalg.lstsq(pr_X[:npoints], pr_y[:npoints], rcond=None)[0]  \n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    pr_fy = np.zeros(len(pr_x))\n",
    "    for k in range(len(pr_w)):                \n",
    "        ty += pr_w[k]*xx**k\n",
    "    for k in range(poly_degree):\n",
    "        fy += fit_w[k]*xx**k\n",
    "        pr_fy += fit_w[k]*pr_x**k\n",
    "    ty += hidden*np.sin(xx*4)\n",
    "    \n",
    "    \n",
    "    l = np.mean((pr_y-pr_fy)[:npoints]**2)    \n",
    "    lte = np.mean((pr_y-pr_fy)[max_npoints:]**2)\n",
    "    ax0 = ax\n",
    "    ax0.plot(pr_x[:npoints], pr_y[:npoints], 'b.', label=\"train data\")\n",
    "    ax0.plot(pr_x[max_npoints:], pr_y[max_npoints:], 'kx', label=\"test data\")\n",
    "        \n",
    "    if tgt:\n",
    "        ax0.plot(xx, ty, 'b:', label=\"true target\")\n",
    "    if fit:\n",
    "        ax0.plot(xx, fy, 'b--', label=\"best fit\")\n",
    "    \n",
    "    ax0.set_ylim(min(ty)-1, max(ty)+1+noise/2)\n",
    "    ax0.text(0.1,0.15,fr'$\\mathrm{{RMSE}}_\\mathrm{{train}} = ${np.sqrt(l):.3f}', transform=ax0.transAxes, c='r')\n",
    "    ax0.text(0.1,0.05,fr'$\\mathrm{{RMSE}}_\\mathrm{{test}} = ${np.sqrt(lte):.3f}', transform=ax0.transAxes, c='r')\n",
    "    ax0.set_xlabel('$x$')\n",
    "    ax0.set_ylabel('$y$')\n",
    "    ax0.legend(loc=\"upper right\")\n",
    "\n",
    "r\"\"\" TODO learning curve plot! possibly better as a later     \n",
    "    n_samples = np.geomspace(min_npoints, max_npoints, 5, dtype=int)\n",
    "    test_rmse = [np.sqrt(np.mean((pr_X[max_npoints:]@np.linalg.lstsq(pr_X[:n], pr_y[:n], rcond=None)[0] - pr_y[max_npoints:])**2))\n",
    "                for n in n_samples]\n",
    "    train_rmse = [np.sqrt(np.mean((pr_X[:n]@np.linalg.lstsq(pr_X[:n], pr_y[:n], rcond=None)[0] - pr_y[:n])**2))\n",
    "            for n in n_samples]\n",
    "    ax[1].plot(n_samples, test_rmse, marker='o', color='b', label='test')\n",
    "    ax[1].plot(n_samples, train_rmse, marker='x', color='black', label='train')\n",
    "    ax[1].set_xlabel('$n_\\mathrm{train}$')\n",
    "    ax[1].set_ylabel('$\\mathrm{{RMSE}}$')\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "    ax[1].set_ylim(0, 6.5)\n",
    "    ax[1].set_xlim(0, max_npoints)\n",
    "\"\"\"\n",
    "    \n",
    "pr_loss_parameters = ParametersPanel(    \n",
    "        noise=FloatSlider(value=5.0, min=0.1, max=10, step=0.1, description='Noise'),\n",
    "        hidden=FloatSlider(value=0.0, min=0, max=5, step=0.01, description='Hidden', readout_format= \".2f\"),\n",
    "        npoints=IntSlider(value=20, min=5, max=150, step=1, description=r'$n_\\mathrm{train}$'),\n",
    "        tgt=Checkbox(False, description=r'Show true target'),\n",
    "        fit=Checkbox(True, description=r'Show best fit'))    \n",
    "\n",
    "pr_loss_demo = CodeExercise(\n",
    "    parameters=pr_loss_parameters,\n",
    "    outputs=pr_loss_figure,\n",
    "    update=pr_loss_update,\n",
    "    update_mode=\"continuous\"\n",
    ")\n",
    "display(pr_loss_demo)\n",
    "pr_loss_demo.run_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex03a_txt = TextExercise(\n",
    "    description=\"\"\"Compare the error on the train and the test sets. \n",
    "    Which is typically higher? How do train and test errors change when you \n",
    "    change number of training points from the lowest to the highest level?\"\"\",\n",
    "    key=\"03a\",\n",
    "    title=\"Exercise 03a: Train and test errors along a learning curve\",\n",
    "    exercise_registry=exercise_registry,\n",
    ")\n",
    "display(ex03a_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex03b_txt = TextExercise(\n",
    "    description=\"\"\"How do the train and test loss change when the level of \n",
    "    noise is increased? And how do they change when the level of hidden relationships \n",
    "    is increased or decreased? Is there a clear difference between the effect of \n",
    "    noise and that of hidden terms?\"\"\",\n",
    "    key=\"03b\",\n",
    "    title=\"Exercise 03b: Effect of noisy data\",\n",
    "    exercise_registry=exercise_registry,\n",
    ")\n",
    "display(ex03b_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tendency of achieving a very low loss on the train set and a much larger test set error is a general phenomenon known as [_overfitting_](https://en.wikipedia.org/wiki/Overfitting). Overfitting is usually particularly bad when the train set size siginficant smalller than the number of model parameters. Polynomial regression with a high degree is notorious for overfitting.\n",
    "\n",
    "A common strategy to avoid overfitting is known as _regularization_. In broad terms, regularization implies penalizing solutions of the model that are too rapidly varying, and favouring those that are smoother, even at the cost of a slight increase of the train set error. In linear regression, the most common approach for linear models is to introduce a penalty term on the variance of the model. The variance of a linear model is determined by the parameter space the weights can span. Therefore a penalty term on the norm of the weights is added to the loss function. The most popular case of the $L^2$-norm is referred to as the [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization). In this case the loss function is extended by a $L^2$ penalty on the weights.\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i (\\mathbf{w}\\cdot \\mathbf{x}_i - y_i)^2 + \\lambda \\|\\mathbf{w}\\|^2_2.\n",
    "$$\n",
    "\n",
    "The model corresponding to this loss function is referred to as $L^2$-regularized least-squares estimator or ridge regression. The $L^2$-regularization is the most popular approach, because it yields a closed-form solution for the weights \n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X}+\\lambda \\mathbf{1})^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "\n",
    "and can therefore be efficiently computed. Alternative choices of norm, both in the error and in the regularization, may lead to desirable behaviors. For example the $L^1$ norm, referred to as [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)), is typically used to obtain a _sparse_ weight matrix, i.e. a matrix in which many terms are vanishingly small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This widget allows you to experiment with the effect of ridge regularization on the same polynomial fitting exercise. \n",
    "\n",
    "_NB: given that we are using a very poor basis, and different features span widely different scales, the underlying implementation is slightly more complicated, in that different weights are scaled differently before computing the Tikhonov term. This scaling is done so that a single parameter can be meaningfully used to control the regularity of the fit._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_demo_figure, _ = plt.subplots(1, 1, tight_layout=True)\n",
    "\n",
    "def regularization_update(code_exercise):\n",
    "    noise, hidden, npoints, tgt, fit, lam = code_exercise.parameters.values()\n",
    "    ax = code_exercise.figure.get_axes()[0]       \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    poly_degree = 6\n",
    "    np.random.seed(54321)\n",
    "    xsz = 10\n",
    "    pr_x = (np.random.uniform(size=2*npoints)-0.5)*xsz\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    pr_y = (np.random.uniform(size=len(pr_x))-0.5)*noise\n",
    "    for k in range(len(pr_w)):\n",
    "        pr_y += pr_w[k]*pr_x**k\n",
    "    pr_y += hidden*np.sin(pr_x*4)\n",
    "    wscale = np.asarray([1/(xsz*0.5)**k for k in range(npoly+1)])\n",
    "    fit_w = np.linalg.solve(\n",
    "        pr_X[::2].T@pr_X[::2]+\n",
    "        10**lam*(npoints//2)*np.diag(wscale**2), \n",
    "        pr_X[::2].T@pr_y[::2])\n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    pr_fy = np.zeros(len(pr_x))\n",
    "    for k in range(len(pr_w)):                \n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        pr_fy += fit_w[k]*pr_x**k\n",
    "    ty += hidden*np.sin(xx*4)\n",
    "    \n",
    "    l = np.mean((pr_y-pr_fy)[::2]**2)\n",
    "    lte = np.mean((pr_y-pr_fy)[1::2]**2)\n",
    "    ax.plot(pr_x[::2], pr_y[::2], 'b.', label=\"train data\")\n",
    "    ax.plot(pr_x[1::2], pr_y[1::2], 'kx', label=\"test data\")    \n",
    "    \n",
    "    if tgt:\n",
    "        ax.plot(xx, ty, 'b:', label=\"true target\")\n",
    "    if fit:\n",
    "        ax.plot(xx, fy, 'b--', label=\"best fit\")\n",
    "    \n",
    "    ax.set_ylim(min(ty)-1, max(ty)+1+noise/2)    \n",
    "    ax.text(0.1,0.25,fr'$\\mathrm{{RMSE}}_\\mathrm{{train}} = ${np.sqrt(l):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.15,fr'$|\\mathbf{{w}}| = ${np.linalg.norm(wscale*fit_w):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.05,fr'$\\mathrm{{RMSE}}_\\mathrm{{test}} = ${np.sqrt(lte):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    \n",
    "regularization_pb = ParametersPanel(    \n",
    "    noise=FloatSlider(value=5.0, min=0.1,max=10,step=0.1, description='Noise'),\n",
    "    hidden=FloatSlider(value=0.0, min=0, max=5,step=0.01, description='Hidden', readout_format=\".2f\"),\n",
    "    npoints=IntSlider(value=10, min=5, max=100, step=1, description=r'$n_\\mathrm{train}$'),\n",
    "    tgt=Checkbox(value=True, description=r'Show true target'),\n",
    "    fit=Checkbox(value=True, description=r'Show best fit'),\n",
    "    lam = FloatSlider(value=-5.0,min=-5,max=5,step=0.1, description=r'$\\log_{10} \\lambda$'))\n",
    "\n",
    "regularization_demo = CodeExercise(\n",
    "    parameters=regularization_pb,\n",
    "    outputs=regularization_demo_figure,\n",
    "    update = regularization_update)\n",
    "\n",
    "regularization_demo.run_update()\n",
    "display(regularization_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex04a_txt = TextExercise(\n",
    "    description=mdwn(r\"\"\"Work with \n",
    "    `(noise, hidden, ntrain) = (5,0,10)`. \n",
    "    What is the value of $\\lambda$ that minimizes the _test_ error?\n",
    "    \"\"\"),\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"04a\",\n",
    "    title=\"Exercise 04a: Optimal regularization\"\n",
    ")\n",
    "display(ex04a_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex04b_txt = TextExercise(\n",
    "    description=\"\"\"\n",
    "    Working with the same parameters (most importantly, the number of train points), \n",
    "    comment on the behavior of the best fit function (smoothness, maximum error \n",
    "    from a training data point) and the various diagnostics as you vary the \n",
    "    regularization away from the optimum value.\n",
    "    \"\"\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"04b\",\n",
    "    title=\"Eercise 04b: From overfitting to underfitting\"\n",
    ")\n",
    "display(ex04b_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex04c_txt = TextExercise(\n",
    "    description=r\"\"\"\n",
    "    Increase the number of training points to 100. How does the behavior of \n",
    "    ridge regression change? Is the same value of $\\lambda$ still optimal? \n",
    "    \"\"\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"04c\",\n",
    "    title=\"Exercise 04c: Give me more data!\"\n",
    ")\n",
    "display(ex04c_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization $\\lambda$ is one of the so-called _hyperparameters_ (\"hypers\"), that tune the behavior of the model but are not directly optimized on the train set. In this case, the number of polynomial terms is another hyperparameter. Optimizing the hyperparameters on the test set is bad practice, because this amounts to _data leakage_, and makes the test error less representative of the true generalization capabilities of the model. \n",
    "\n",
    "We won't get into details, but consider that the strategy to optimize the hyperparameters is to set aside a _validation_ set that is neither used for training nor for testing, but just to tune the hyperparameter values, or to perform _cross validation_. This implies splitting the training set into train/validation parts, and repeating the exercise over multiple _folds_ (subsets of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprints and descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in any data-driven study of materials involves codifying the structure and composition of the materials being studied into a mathematical form that is suitable to be used as the input of the subsequent steps. Here we focus in particular on the definition of _fingerprints_, or _descriptors_ - a vector of numbers that are associated with each structure, assembled into a _feature vector_ $\\mathbf{x}_i$. \n",
    "\n",
    "In this module we are going to use a dataset of materials from the [materials project](https://materialsproject.org/). The dataset has been reformatted as an extended XYZ file, that can be read with the ASE `ase.io.read` function. The target properties (mostly related to the elastic behavior) can be accessed from the `structure.info` dictionary of each frame.\n",
    "\n",
    "```\n",
    "data = read(\"filename.xyz\", \":\")\n",
    "X = []\n",
    "y = []\n",
    "for structure in data:\n",
    "    X.append(get_features(structure))\n",
    "    y.append(structure.info['property_name'])\n",
    "```\n",
    "\n",
    "where `get_features` is a function that processes the structural information of each frame to convert it into a set of fingerprints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptors can be either precise [_representations_](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00021) of the coordinates and chemical nature of all atoms in a structure (which are commonplace in the construction of machine-learning interatomic potentials) or fingerprints based on a combination of structural parameters and properties that can be computed easily. For instance, one could take the electronegativity of elements in combination with the point group of the crystal structure. \n",
    "\n",
    "Here we take a very simple (and somewhat crude) approach, describing each structure by its chemical composition - that is, the feature $x_{iZ}$ contains the fraction of the atoms of structure $i$ that has atomic number $Z$. This means that for instance $\\mathrm{H_2}$ would have fingerprint `[1,0,0,0, ... ]`, $\\mathrm{LiH}$ `[0.5,0,0.5,0,0, ... ]`, $\\mathrm{Li_2HBe}$ `[0.25,0,0.5,0.25, ... ]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptor_base(structures):\n",
    "    \"\"\"\n",
    "    Computes a feature matrix for the structures given in input,\n",
    "    which is a vector of the fractional composition of each structure in the dataset, \n",
    "    e.g. given [H2, He2, HHe, LiH] returns something like \n",
    "    [[1,0,0],[0,1,0],[0.5,0.5,0],[0.5,0,0.5]]. \n",
    "    The total vector size depends on how many elements are present in the data set,\n",
    "    but it's OK if there are zeros. \n",
    "    \n",
    "    :param structures: a list of ase.Atoms structures\n",
    "    \n",
    "    :return: a (nstructures x nspecies) feature matrix containing the composition fingerprints\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from ase.io import read\n",
    "    \n",
    "    # allocates plenty of space for all elements\n",
    "    descriptor = np.zeros((len(structures), 100))\n",
    "    \n",
    "    # fill the descriptor with composition features\n",
    "    # NB: structures[i].numbers contains a list of the atomic numbers of the atoms in the structure\n",
    "    \n",
    "    return descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_html_table(numpy_array, header):\n",
    "    rows = \"\"\n",
    "    for i in range(len(numpy_array)):\n",
    "        rows += \"<tr>\" + functools.reduce(lambda x,y: x+y,\n",
    "                             map(lambda x: \"<td>\" + str(x) + \"</td>\",\n",
    "                                 numpy_array[i])\n",
    "                            ) + \"</tr>\"\n",
    "\n",
    "    return \"<table>\" + header + rows + \"</table>\"\n",
    "\n",
    "\n",
    "def mk_table_05(code_example):\n",
    "    structures=read('data/mp_elastic.extxyz',':')\n",
    "    l = code_example.code(structures)\n",
    "\n",
    "    x = []   \n",
    "    for a,b in enumerate(l):\n",
    "        s = structures[a].symbols\n",
    "        x.append([s,np.round(b,2).tolist()])\n",
    "\n",
    "    header = \"\"\"<tr>\n",
    "                  <th>Symbols <span style=\"padding-left:150px\"></th>\n",
    "                  <th>Fractional composition <span style=\"padding-left:150px\"></th>\n",
    "                </tr>\"\"\"\n",
    "    demo_table_html = HTML(\n",
    "        value=f\"Table\")\n",
    "    demo_table_html.value = array_to_html_table(x, header)\n",
    "\n",
    "    demo_table = HBox(layout=Layout(height='270px', overflow_y='auto'))\n",
    "    demo_table.children += (demo_table_html,) \n",
    "    display(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex05_assert_equal(a,b):\n",
    "    # checks if the Gram matrix is the same, so we avoid differences in order and zeros    \n",
    "    return np.allclose(a@a.T, b@b.T)\n",
    "\n",
    "def table_updater(code_example):\n",
    "    with code_example.output:\n",
    "        mk_table_05(code_example)\n",
    "        \n",
    "ex05_code_demo = CodeExercise(\n",
    "    code=descriptor_base,\n",
    "    check_registry=check_registry,\n",
    "    outputs=CueObject(),\n",
    "    update=table_updater,\n",
    "    key=\"05\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    title=\"Exercise 05: Composition fingerprints\",\n",
    "    description=mdwn(r\"\"\"\n",
    "Write a function that takes structural information for each frame and returns \n",
    "a vector containing the fractional composition of each compound, to be used \n",
    "as descriptors.\n",
    "\n",
    "_Hint_: ASE `Atoms` frames have a `numbers` member variable that holds an \n",
    "array with the atomic numbers of all atoms in the structure (e.g. a $\\mathrm{CH_4}$ \n",
    "structure will have `frame.numbers == [6,1,1,1,1]`).\n",
    "    \"\"\"),\n",
    "    update_mode=\"manual\"\n",
    ")\n",
    "\n",
    "ex05_reference01 = read('data/mp_elastic.extxyz','::100')\n",
    "ex05_ref_input = [{\"structures\" : ex05_reference01}]\n",
    "ref_output = np.loadtxt('data/mp_elastic_05ref.txt')\n",
    "ex05_ref_output_fp = [(ref_output@ref_output.T,) ]\n",
    "\n",
    "check_registry.add_check(ex05_code_demo,\n",
    "    asserts=[\n",
    "        assert_type,\n",
    "        assert_numpy_allclose,\n",
    "    ],\n",
    "    inputs_parameters=ex05_ref_input,\n",
    "    outputs_references=ex05_ref_output_fp,\n",
    "    fingerprint=(lambda x:x@x.T)) # compares the Gram matrix so results don't depend on shape or zeros\n",
    "#    equal=ex05_equality)\n",
    "display(ex05_code_demo)\n",
    "ex05_code_demo.run_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**06** Extend the function above to also evaluate powers of the fractional compositions, and combine them in a single feature vector (i.e. to have $[x_\\mathrm{H}, x_\\mathrm{He}, \\ldots x_\\mathrm{U}, x_\\mathrm{H}^2, x_\\mathrm{He}^2 , \\ldots, x_\\mathrm{H}^{n_\\text{max}}, x_\\mathrm{He}^{n_\\text{max}} , \\ldots]$).</span>\n",
    "\n",
    "_Hint: you can use `np.hstack([x1, x2, x3])` to combine several matrices row-wise (the columns of `x2` and `x3` are pasted to the right of those of `x1`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptor_poly(structures, nmax):\n",
    "    \"\"\"\n",
    "    compute the powers of the fractional composition and stack them \n",
    "    to form a larger feature vector\n",
    "    \n",
    "    structures: a list of structures in `ase.Atoms` format\n",
    "    nmax : maximum order of the polynomial (descriptors should be computed for \n",
    "           exponents 1 to nmax)\n",
    "    \n",
    "    returns: a n_structures*(n_features*nmax) array containing the concatenated \n",
    "             polynomial features\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from ase.io import read\n",
    "    \n",
    "    # allocates plenty of space for all elements\n",
    "    descriptor = np.zeros((len(structures), 100))\n",
    "    \n",
    "    # fill the descriptor with composition features\n",
    "    # NB: structures[i].numbers contains a list of the atomic numbers of the atoms in the structure\n",
    "    \n",
    "    \n",
    "    # create a list containing the entry-wise powers of the compositions\n",
    "    # NB: if X is a numpy array, X**k computes the k-th entry-wise power\n",
    "    # NB: you can use `np.hstack` to combine different matrices into a bigger array. Read the doc!\n",
    "    descriptor = np.hstack( LIST_OF_MATRICES )\n",
    "    return descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_table_06(code_exercise):\n",
    "    structures=read('data/mp_elastic.extxyz',':')\n",
    "    l = code_exercise.code(structures, ex06_wp.parameters['n'])\n",
    "\n",
    "    x = []   \n",
    "    for a,b in enumerate(l):\n",
    "        s = structures[a].symbols\n",
    "        x.append([s,np.round(b,2).tolist()])\n",
    "\n",
    "    header = \"\"\"<tr>\n",
    "                  <th>Symbols <span style=\"padding-left:150px\"></th>\n",
    "                  <th>Fractional composition up to order n <span style=\"padding-left:150px\"></th>\n",
    "                </tr>\"\"\"\n",
    "    demo_table_html = HTML(\n",
    "        value=f\"Table\")\n",
    "    demo_table_html.value = array_to_html_table(x[::100], header)\n",
    "\n",
    "    demo_table = HBox(layout=Layout(height='250px', overflow_y='auto'))\n",
    "    demo_table.children += (demo_table_html,) \n",
    "    display(demo_table)\n",
    "    \n",
    "ex06_wp =  ParametersPanel(n = IntSlider(value=2,min=1,max=8,description=r'$n_{max}$'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_updater2(code_exercise):\n",
    "    with code_exercise.output:\n",
    "        mk_table_06(code_exercise)\n",
    "\n",
    "\n",
    "ex06_code_demo = CodeExercise(\n",
    "    code=descriptor_poly,\n",
    "    parameters=ex06_wp,\n",
    "    check_registry=check_registry,\n",
    "    outputs= [CueObject()],\n",
    "    update = table_updater2,\n",
    "    key=\"06\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    title=\"Exercise 06: Polynomial descriptors\",\n",
    "    description=mdwn(r\"\"\"\n",
    "Extend the function above to also evaluate powers of the \n",
    "fractional compositions, and combine them in a single feature \n",
    "vector (i.e. to have \n",
    "$[x_\\mathrm{H}, x_\\mathrm{He}, \\ldots x_\\mathrm{U}, x_\\mathrm{H}^2, \n",
    "x_\\mathrm{He}^2 , \\ldots, x_\\mathrm{H}^{n_\\text{max}}, x_\\mathrm{He}^{n_\\text{max}} , \\ldots]$).\n",
    "\n",
    "_Hint:_ you can use `np.hstack([x1, x2, x3])` to combine several matrices row-wise \n",
    "(the columns of `x2` and `x3` are pasted to the right of those of `x1`).\n",
    "    \"\"\"),\n",
    "    update_mode=\"manual\"\n",
    ")\n",
    "\n",
    "\n",
    "ex06_reference01 = read('data/mp_elastic.extxyz','::100')\n",
    "ex06_ref_input = [{\"structures\" : ex06_reference01, \"nmax\":3}]\n",
    "ref_output = np.loadtxt('data/mp_elastic_06ref.txt')\n",
    "ex06_ref_output_fp = [(ref_output@ref_output.T,) ]\n",
    "\n",
    "check_registry.add_check(\n",
    "    ex06_code_demo,\n",
    "    asserts=[\n",
    "        assert_type,\n",
    "        assert_numpy_allclose,\n",
    "    ],\n",
    "    inputs_parameters=ex06_ref_input,\n",
    "    outputs_references=ex06_ref_output_fp,\n",
    "    fingerprint=(lambda x:x@x.T)  # compares the Gram matrix so results don't depend on shape or zeros\n",
    ")\n",
    "\n",
    "display(ex06_code_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL** \n",
    "\n",
    "Here you can define your own fingerprints. Number density? Mass? Element electronegativity? Use your imagination. You will be able to test the performance of these descriptors in the next sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptor_custom(structures):\n",
    "    \"\"\"computes a custom feature matrix for the structures given in input.\"\"\"\n",
    "    import numpy as np\n",
    "    from ase.io import read\n",
    "    \n",
    "    descriptor = np.zeros((len(structures), 10))\n",
    "    \n",
    "    return descriptor\n",
    "\n",
    "def mk_table_custom(code_exercise):\n",
    "    print_output = code_exercise.output\n",
    "    structures=read('data/mp_elastic.extxyz',':')\n",
    "    l = code_exercise.code(structures)\n",
    "\n",
    "    x = []   \n",
    "    for a,b in enumerate(l):\n",
    "        s = structures[a].symbols\n",
    "        x.append([s,np.round(b,2).tolist()])\n",
    "\n",
    "    header = r\"\"\"<tr>\n",
    "                  <th>Symbols <span style=\"padding-left:150px\"></th>\n",
    "                  <th>Custom descriptors<span style=\"padding-left:150px\"></th>\n",
    "                </tr>\"\"\"\n",
    "    demo_table_html = HTML(\n",
    "        value=f\"Table\")\n",
    "    demo_table_html.value = array_to_html_table(x[::100], header)\n",
    "\n",
    "    demo_table = HBox(layout=Layout(height='250px', overflow_y='auto'))\n",
    "    demo_table.children += (demo_table_html,) \n",
    "    with print_output:\n",
    "        display(demo_table)\n",
    "\n",
    "\n",
    "custom_demo = CodeExercise(\n",
    "    code=descriptor_custom,\n",
    "    outputs= [CueObject()],\n",
    "    update = mk_table_custom,\n",
    "    key=\"custom\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    title=\"Optional: Build your own descriptor\",\n",
    "    description=\"\"\"\n",
    "Here you can define your own fingerprints. Number density? Mass? \n",
    "Element electronegativity? Use your imagination. You will be able \n",
    "to test the performance of these descriptors in the next sections. \n",
    "\"\"\" \n",
    ")\n",
    "\n",
    "custom_demo.run_update()\n",
    "display(custom_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a quick example of principal components analysis (PCA)- one of the simplest _unsupervised_ learning algorithms - actually, one that can hardly be called machine learning. \n",
    "PCA involves processing a high-dimensional feature matrix $\\mathbf{X}$ and projecting it _linearly_ into a _latent space_ $\\mathbf{T}$ of reduced dimensionality. The problem can be formulated in different terms: as the identification of the directions with maximum variance in feature space, as the maximisation of the variance retained in the latent space, or as the low-rank orthogonal projection of the feature matrix that minimizes the information loss. \n",
    "\n",
    "The figure below demonstrates the functioning of PCA on a simple 2D dataset: the principal axes of the data distribution are identified, making it possible to reduce the description to just 1D while losing the smallest possible amount of information on the relative position of the points\n",
    "\n",
    "<center><img src=\"figures/pca.png\" width=\"500\"/></center>\n",
    "\n",
    "In rigorous terms, PCA corresponds to determining the orthogonal projection matrix $\\mathbf{P}_{{XT}}$ that minimizes the loss\n",
    "\n",
    "$$\n",
    "\\ell = \\|\\mathbf{X}-\\mathbf{X}\\mathbf{P}_{XT} \\mathbf{P}_{XT}^T\\|^2_2\n",
    "$$\n",
    "\n",
    "To understand what this does, consider that $\\mathbf{P}_{{XT}}$ is a $n_X\\times n_T$ matrix; $\\mathbf{X}\\mathbf{P}_{XT}$ transforms the feature vector of each point into a $n_T$-dimensional compressed version, forming a latent-space matrix $\\mathbf{T}$. \n",
    "Then $\\mathbf{T}\\mathbf{P}_{XT}^T$ lifts this to the $n_X$-dimensional space. Given the compression, however, information has been lost and the data lies in a subspace (the blue points in the figure above). \n",
    "\n",
    "The projector $\\mathbf{P}_{XT}$ can be built using a [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) of $\\mathbf{X}$ or, equivalently, by computing the eigenvalue decomposition of the covariance matrix $\\mathbf{X}^T\\mathbf{X}$. Note that usually the feature matrix is _centered_ before identifying the principal components; that is, the column-wise average of the features is subtracted from each row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's really easy to implement PCA manually, it is even simpler to use one of the many open implementations available, that also take care of centering. We use the implementation in `scikit-learn`. You are encouraged to read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), but the key workflow is simple:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)   # n_components: dimension of the latent space\n",
    "itrain = range(0,ntrain)  # list of indices used for training\n",
    "pca.fit(x[itrain])      # x[itrain] is a ntrain x nx feature matrix\n",
    "t = pca.transform(x)    # applies compression to the full feature vector. \n",
    "                        # t is nsamples x n_components \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_analysis(structures, f_fingerprint, f_train):\n",
    "    \"\"\"\n",
    "    takes the structures, and the a function that can compute fingerprints, and compute\n",
    "    the principal component analysis of the dataset. also computes a train/test split assuming that \n",
    "    the first len(structures)*f_train configurations are used for training. \n",
    "    \n",
    "    \n",
    "    :param structures: a list of ase.Atoms structures\n",
    "    :param f_fingerprint: a function that takes a list of structures and returns the feature matrix\n",
    "    :param f_train: the fraction of the structures list to be used for training \n",
    "    \n",
    "    :returns: the latent-space coordinates for ALL structures (use at least 4 components), and a list of the indices of the train structures.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from ase.io import read\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # indices of the train set. NB: you can select rows from a numpy array X\n",
    "    # by writing X[itrain]\n",
    "    itrain = list(range(0,int(len(structures)*f_train)))\n",
    "    \n",
    "    # compute the feature matrix for the full set\n",
    "    X = ...\n",
    "    \n",
    "    # initializes the PCA object and calls fit \n",
    "    # on the training set. use at least 4 components, \n",
    "    # as they are needed for the visualization!\n",
    "    \n",
    "    # computes the latent-space projection of the FULL feature matrix\n",
    "    t = ... \n",
    "    return t, itrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_stride = 3\n",
    "def fun_ex08(code_exercise):\n",
    "    feats, ftrain = code_exercise.parameters.values()             \n",
    "    print_output = code_exercise.output\n",
    "    print_output.clear_output()\n",
    "    structures = read('data/mp_elastic.extxyz',':')\n",
    "    with print_output:\n",
    "        if feats == \"composition\":\n",
    "            f_descriptor = lambda s: ex05_code_demo.code(s)\n",
    "        elif feats == \"polynomial ($n_{max}$=2)\":\n",
    "            f_descriptor = lambda s: ex06_code_demo.code(s, 2)\n",
    "        elif feats == \"polynomial ($n_{max}$=4)\":\n",
    "            f_descriptor = lambda s: ex06_code_demo.code(s, 4)\n",
    "        elif feats == \"polynomial ($n_{max}$=8)\":\n",
    "            f_descriptor = lambda s: ex06_code_demo.code(s, 8)\n",
    "        elif feats == \"custom\":\n",
    "            f_descriptor = lambda s: custom_demo.code(s)\n",
    "        xlatent, itrain = code_exercise.code(structures, f_descriptor,  ftrain)\n",
    "    \n",
    "    ftype = np.asarray([ \"test \" ] * len(structures)); ftype[itrain] = \"train\"\n",
    "    fname = [ str(s.symbols) for s in structures]\n",
    "    frames=structures    \n",
    "    properties={\"pca[1]\": xlatent[::cs_stride,0], \"pca[2]\" : xlatent[::cs_stride,1],\n",
    "                 \"pca[3]\" : xlatent[::cs_stride,2],  \"pca[4]\" : xlatent[::cs_stride,3],\n",
    "                \"type\": ftype[::cs_stride] , \"name\": fname[::cs_stride]\n",
    "               }\n",
    "    settings={'map': {'x': {'property': 'pca[1]'},\n",
    "  'y': { 'property': 'pca[2]'},\n",
    "  #'color': {'max': 1, 'min': 0, 'property': 'K_error', 'scale': 'linear'},\n",
    "  'symbol': 'type',\n",
    "  'palette': 'inferno',\n",
    "  'size': {'factor': 40}},\n",
    " 'structure': [{'bonds': True,\n",
    "   'spaceFilling': False,\n",
    "   'atomLabels': False,\n",
    "   'unitCell': True,\n",
    "   'rotation': False,\n",
    "   'supercell': {'0': 2, '1': 2, '2': 2},}]}\n",
    "    \n",
    "    \n",
    "    chemiscope.write_input(\"module_01-pca-analysis.chemiscope.json.gz\", \n",
    "                           frames=frames[::cs_stride], properties=properties)\n",
    "    with print_output:\n",
    "        display(chemiscope.show(frames[::cs_stride], properties, settings=settings\n",
    "                               ))\n",
    "    \n",
    "ex08_pb =  ParametersPanel(feats =\n",
    "    Dropdown(value=\"composition\", \n",
    "             options=[\"composition\", \"polynomial ($n_{max}$=2)\", \"polynomial ($n_{max}$=4)\", \"polynomial ($n_{max}$=8)\", \"custom\"], \n",
    "             description=r\"fingerprints\"),\n",
    "    ftrain = FloatSlider(value=0.5,min=0.05,max=0.9,step=0.05,description=r'$f_{train}$') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def fingerprintf(*args):\n",
    "    return ex05_code_demo.code(*args)\n",
    "def ex08_chk(student,reference):\n",
    "    # takes entry-wise modulus to avoid false-errors due to the \n",
    "    # arbitrariness of signs of the eigenvectors\n",
    "    return np.allclose(np.abs(reference[:2]),np.abs(student[0][:2]))\n",
    "def identity(x):\n",
    "    return x\n",
    "    \n",
    "ex08_code_demo = CodeExercise(\n",
    "    code=PCA_analysis,\n",
    "    parameters=ex08_pb,\n",
    "    check_registry=check_registry,\n",
    "    outputs= [CueObject()],\n",
    "    update = fun_ex08,\n",
    "    key=\"08\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    title=\"Exercise 08: Principal component analysis\",\n",
    "    description=\"\"\"\n",
    "Implement a function that computes the PCA analysis given a set of structures, \n",
    "a fingerprint function that returns the feature matrix, and a the fraction of \n",
    "the structures to be used for training.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "ex08_ref_input = [{\"structures\": read('data/mp_elastic.extxyz','::100'), \"f_fingerprint\": fingerprintf, \"f_train\": 0.5}]\n",
    "ex08_ref_output_fp = [(np.abs(np.loadtxt('data/mp_elastic_08ref.txt')),)]\n",
    "\n",
    "check_registry.add_check(ex08_code_demo,\n",
    "    asserts=[\n",
    "        assert_type,\n",
    "        assert_numpy_allclose,\n",
    "    ],\n",
    "    inputs_parameters=ex08_ref_input,\n",
    "    outputs_references=ex08_ref_output_fp,\n",
    "    fingerprint=(lambda x, i:np.abs(x))  # takes absolute value since eigenvector orientation is random\n",
    ")\n",
    "\n",
    "display(ex08_code_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download chemiscope datafile](./module_01-pca-analysis.chemiscope.json.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex09a_txt = TextExercise(\n",
    "    description=mdwn(\"\"\"\n",
    "Run the PCA for a \"composition\" fingerprint and $f_{train}=0.5$. \n",
    "How does the latent-space projection look like? What do the axes roughly correspond to? \n",
    "How can you explain the appearence of the map, given the way the fingerprint is\n",
    "constructed and the structures in the training set?\n",
    "\n",
    "_Hint:_ Think about what the structures that are shown close together have in common, \n",
    "chemically-speaking, see how the structures change as you examine those that lie at the \n",
    "oppsite edges of the map, and keep in mind what information is encoded in the starting \n",
    "fingerprints.    \n",
    "\"\"\"),\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"09a\",\n",
    "    title=\"Exercise 09a: Latent-space analysis\"\n",
    ")\n",
    "display(ex09a_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex09b_txt = TextExercise(\n",
    "    description=mdwn(\"\"\"\n",
    "Change the fraction of  training structures down to 0.05; does the qualitative appearence of \n",
    "the map change much? Do the actual meaning of the axes change (look in particular at the most \n",
    "\"extreme\" structures, that occurr at the periphery of the map)?\n",
    "\"\"\"),\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"09b\",\n",
    "    title=\"Exercise 09b: Data poor regime \"\n",
    ")\n",
    "display(ex09b_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex09c_txt = TextExercise(\n",
    "    description=mdwn(r\"\"\"\n",
    "What happens if you use another set of features (say polynomial features \n",
    "up to $n_\\mathrm{max}=8$)? Go back to 50% training.\n",
    "\"\"\"),\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"09c\",\n",
    "    title=\"Exercise 09c: Polynomial features\"\n",
    ")\n",
    "display(ex09c_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we attempt to establish an explicit data-driven relationship between the fingerprints we use to describe the structures in the dataset and their physical properties - in particular the Young modulus $K$ (the shear modulus $G$ and the Poisson ratio $\\nu$ can also be chosen, if you want to try). \n",
    "\n",
    "We will use Ridge Regression, which is exactly the same framework we discussed in [section 1](#data-driven) in general terms. The loss is defined as \n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i | \\mathbf{w}\\cdot \\mathbf{x}_i - y_i | ^2 + \\alpha |\\mathbf{w}|^2.\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}$ are the regression weights, $y_i$ the target property for each sample and $\\mathbf{x}_i$ the features associated with that sample. The regularization term $\\alpha |\\mathbf{w}|^2$ penalizes solutions with large values of the weights, and usually leads to a smoother interpolant of the training data that is less susceptible to overfitting. \n",
    "\n",
    "Even though - much as with PCA - it is simple to implement a solver for ridge regression, we will use the subroutines implemented in `scikit-learn`, namely `sklearn.linear_model.Ridge` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html))\n",
    "\n",
    "Usage is simple \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1e-4)   # alpha: magnitude of ridge regularization \n",
    "itrain = range(0,ntrain)    # list of indices used for training\n",
    "ridge.fit(x[itrain], y[itrain])  # x[itrain] is a ntrain x nx feature matrix. \n",
    "                                 # y[itrain] is a vector of targets\n",
    "y_pred = ridge.predict(x)    # predicts based on the full feature vector                        \n",
    "```\n",
    "\n",
    "_NB: it is good practice to randomize the entries before splitting train and test set, which we skip here for simplicity. You can see [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for a discussion and some utility functions from `sklearn`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(structures, target, f_fingerprint, f_train, alpha):\n",
    "    \"\"\"\n",
    "    takes the structures, and a function that can compute fingerprints, and compute\n",
    "    linear regression for the target. the target is given as a string, matching the name used in \n",
    "    the `info` field in the structures.\n",
    "    also computes a train/test split assuming that the first len(structures)*f_train configurations are used for training. \n",
    "    takes the ridge regularization magnitude alpha as input\n",
    "    \n",
    "    :param structures: a list of ase.Atoms structures\n",
    "    :param target: a string indicating which property to learn\n",
    "    :param f_fingerprint: a function that takes a list of structures and returns the feature matrix\n",
    "    :param f_train: the fraction of the structures list to be used for training \n",
    "    :param alpha: the ridge regularization\n",
    "    \n",
    "    :returns: the latent-space coordinates for ALL structures (use at least 4 components), and a list of the indices of the train structures.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from ase.io import read\n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    # indices of the train set. NB: you can select rows from a numpy array X\n",
    "    # by writing X[itrain]\n",
    "    itrain = list(range(0,int(len(structures)*f_train)))\n",
    "    \n",
    "    # compute the feature matrix for the full set\n",
    "    X = ...\n",
    "    # makes a list of the targets, by extracting .info[target] from each structure\n",
    "    y = ...\n",
    "    # NB: if you use a Python list, convert it to a numpy array so you can index it with itrain\n",
    "    # e.g. if y = [1,2,3,4] you can't do y[itrain], but you can if y = np.asarray([1,2,3,4])\n",
    "    \n",
    "    # initializes the Ridge object and calls fit on the training set\n",
    "    \n",
    "    # predicts the property for ALL structures \n",
    "    y_pred = ... \n",
    "    return y_pred, itrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_stride = 3\n",
    "def fun_ex10(code_exercise):\n",
    "    tgt,feats,ftrain,log10alpha = code_exercise.parameters.values()\n",
    "    print_output = code_exercise.outputs[0]\n",
    "    print_output.clear_output()\n",
    "    structures = read('data/mp_elastic.extxyz',':')\n",
    "    y = np.asarray([f.info[tgt] for f in structures])\n",
    "    with print_output:\n",
    "        if feats == \"composition\":\n",
    "            f_descriptor = lambda s: ex05_code_demo.code(s)\n",
    "        elif feats == \"polynomial ($n_{max}$=2)\":\n",
    "            f_descriptor = lambda s: ex06_code_demo.code(s, 2)\n",
    "        elif feats == \"polynomial ($n_{max}$=4)\":\n",
    "            f_descriptor = lambda s: ex06_code_demo.code(s, 4)\n",
    "        elif feats == \"polynomial ($n_{max}$=8)\":\n",
    "            f_descriptor = lambda s: ex06_code_demo.code(s, 8)\n",
    "        elif feats == \"custom\":\n",
    "            f_descriptor = lambda s: custom_demo.code.get_function_object()(s)\n",
    "        \n",
    "        yp, itrain = code_exercise.code(structures, tgt, f_descriptor,  \n",
    "                                                    ftrain, 10**log10alpha)\n",
    "        \n",
    "        print(\"MAE train: \", np.mean(np.abs((y-yp)[itrain])))\n",
    "        print(\"MAE test: \",(np.sum(np.abs(y-yp)) - np.sum(np.abs(y-yp)[itrain]))/(len(y)-len(itrain)) )\n",
    "    \n",
    "    ftype = np.asarray([ \"test \" ] * len(structures)); ftype[itrain] = \"train\"\n",
    "    fname = [ str(s.symbols) for s in structures]\n",
    "    frames=structures\n",
    "    properties={tgt: y[::cs_stride], tgt+\"_predicted\" : yp[::cs_stride], tgt+\"_error\": np.abs(y-yp)[::cs_stride],\n",
    "                \"type\": ftype[::cs_stride] , \"name\": fname[::cs_stride]}\n",
    "    \n",
    "    settings={'map': {'x': {'property': tgt},\n",
    "  'y': { 'property': tgt+'_predicted'},\n",
    "  'color': {'max': 1, 'min': 0, 'property': tgt+'_error', 'scale': 'linear'},\n",
    "  'symbol': 'type',\n",
    "  'palette': 'inferno',\n",
    "  'size': {'factor': 40}},\n",
    " 'structure': [{'bonds': True,\n",
    "   'spaceFilling': False,\n",
    "   'atomLabels': False,\n",
    "   'unitCell': True,\n",
    "   'rotation': False,\n",
    "   'supercell': {'0': 2, '1': 2, '2': 2},}]}\n",
    "                  \n",
    "    chemiscope.write_input(\"module_01-ridge-regression.chemiscope.json.gz\", frames=frames[::cs_stride], properties=properties)\n",
    "                           \n",
    "    with print_output:\n",
    "        display(chemiscope.show(frames=structures[::cs_stride], \n",
    "                   properties=properties, settings=settings\n",
    "                  ) )\n",
    "        \n",
    "ex10_pb =  ParametersPanel(\n",
    "    target=Dropdown(value=\"K\", options=[\"K\", \"G\", \"nu\"], description=r\"target\"),\n",
    "    feats=Dropdown(value=\"composition\", options=[\"composition\", \"polynomial ($n_{max}$=2)\", \"polynomial ($n_{max}$=4)\", \"polynomial ($n_{max}$=8)\", \"custom\"], \n",
    "                   description=r\"fingerprints\"),\n",
    "    ftrain = FloatSlider(value=0.5,min=0.05,max=0.9,step=0.05,description=r'$f_{train}$'),\n",
    "    log10alpha=FloatSlider(value=-3.,min=-8, max=2, step=0.2, description=r\"$\\log_{10}(\\alpha)$\")\n",
    "                       )\n",
    "def fingerprintf(*args):\n",
    "    return ex05_code_demo.code(*args)\n",
    "def ex10_chk(a,b):\n",
    "    return np.allclose(a[0],b[0])\n",
    "ex10_reference_input = [{'structures':read('data/mp_elastic.extxyz','::100'),'target':\"K\", 'f_fingerprint' :fingerprintf,'f_train':0.5,'alpha':1e-3}]\n",
    "ex10_reference_output = [(np.loadtxt('data/mp_elastic_10ref.txt'),)]\n",
    "ex10_code_demo = CodeExercise(\n",
    "            code=ridge_regression,\n",
    "            check_registry=check_registry,\n",
    "            parameters=ex10_pb,\n",
    "            outputs= [CueObject()],\n",
    "            update = fun_ex10,\n",
    "    key=\"10\",\n",
    "    exercise_registry=exercise_registry,\n",
    "    title=\"Exercise 10: Ridge regression\",\n",
    "    description=mdwn(r\"\"\"\n",
    "Implement a function that fits and evaluates a ridge regression model for a set of structures. \n",
    "The string indicating which property should be fitted, a fingerprint function that returns the \n",
    "feature matrix, and a list of the indices of the structures used for training.\n",
    "The fraction of the structures to be used for training, are also arguments of the function.\n",
    "\n",
    "_NB:_ the widget computes the mean absolute error (MAE), $\\sum_i |y_i - y(x_i)|$, which is a \n",
    "measure of error that is less sensitive to outlier values than the root mean square. \n",
    "\"\"\")\n",
    ")\n",
    "check_registry.add_check(ex10_code_demo,\n",
    "    asserts= [\n",
    "        assert_type,\n",
    "        assert_numpy_allclose,\n",
    "    ],\n",
    "     inputs_parameters=ex10_reference_input,\n",
    "     outputs_references =ex10_reference_output,\n",
    "     fingerprint=lambda x,y: x)\n",
    "display(ex10_code_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download chemiscope datafile](./module_07-ridge-regression.chemiscope.json.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex11a_txt = TextExercise(\n",
    "    description=mdwn(r\"\"\"\n",
    "Run regression for the Young modulus $K$, using `composition` fingerprints, \n",
    "50% training structures and a regularization of $10^{-3}$. What are the train and test errors?\n",
    "\"\"\"),\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"11a\",\n",
    "    title=\"Exercise 11a: Learning the Young modulus\"\n",
    ")\n",
    "display(ex11a_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex11b_txt = TextExercise(\n",
    "    description=mdwn(\"\"\"\n",
    "Change the train size to the minimum and maximum values allowed. How do test and train mean \n",
    "absolute errors (MAEs) change? Can you explain the trend? Repeat with very small and very \n",
    "large regularization. What do you observe, and how can you explain it?\"\"\"),\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"11b\",\n",
    "    title=\"Exercise 11b: Learning curve\"\n",
    ")\n",
    "display(ex11b_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex11c_txt = TextExercise(\n",
    "    description=mdwn(r\"\"\"\n",
    " Use polynomial ($n_{max}=8$) descriptors and repeat the experiments in the previous question. \n",
    " What do you observe, and how can you explain it? What is the best test set error you can obtain \n",
    " by adjusting the regularization with `f_train=0.85`?\n",
    "\n",
    "_Hint:_ consider the number of features used in the description, and the flexibility of the model. \n",
    "Think also at what you observed for the 1D regression. Remember: adjusting the regularization based on \n",
    "test set accuracy as we do here is not good practice, and we only do it to get a sense of the \n",
    "role of regularization.\n",
    "\"\"\"),\n",
    "    exercise_registry=exercise_registry,\n",
    "    key=\"11c\",\n",
    "    title=\"Exercise 11c: Best regression model\"\n",
    ")\n",
    "display(ex11c_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve the custom descriptors to achieve a more accurate prediction of the Young modulus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "461px",
    "left": "0px",
    "right": "927.667px",
    "top": "107px",
    "width": "139px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
